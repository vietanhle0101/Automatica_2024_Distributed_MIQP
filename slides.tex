\documentclass[9pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{comment}
%package for algorithm/pseudo code
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\usetheme{Madrid}
\usecolortheme{default}

\usepackage{xspace}
\usepackage[style=authortitle ,backend=biber]{biblatex}
\addbibresource{references.bib}
% \addbibresource{references/IDS_publication.bib}

\usepackage{todonotes}

% Algorithms
% \usepackage[ruled]{algorithm}
% \usepackage[noend]{algpseudocode}

\usepackage[extramath,probability]{mylatexdefs}

\input{defs}

% All user-defined commands should be in defs.tex

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[Research] %optional
{Technical Notes for Research Project} 

\subtitle{Summary}

\author[Viet-Anh Le] % (optional)
{Viet-Anh Le\inst{1,2}}

\institute[UD] % (optional)
{
\inst{1}%
University of Delaware
\\
\inst{2}%
Cornell University
}

%\date[VLC 2021] % (optional)
%{Very Large Conference, April 2021}

%\logo{\includegraphics[height=1cm]{overleaf-logo}}

%End of title page configuration block
%------------------------------------------------------------


%------------------------------------------------------------
%The next block of commands puts the table of contents at the
%beginning of each section and highlights the current section:

% \AtBeginSection[]
% {
%   \begin{frame}
%     \frametitle{Table of Contents}
%     \tableofcontents[currentsection]
%   \end{frame}
% }
%-----------------------------------------------------------
\begin{document}

%The next statement creates the title page.
\frame{\titlepage}


%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
% \begin{frame}
% \frametitle{Table of Contents}
% \tableofcontents
% \end{frame}
%---------------------------------------------------------

\begin{frame}{Distributed Optimization}
\begin{itemize}
\item Consider an optimization problem involving $N$ agents with separable objectives and coupling constraints:
\begin{equation}
\begin{split}
\underset{\bbsym{x}_i \in \XXX_i^{\rm{MI}}}{\min} & \quad \sum_{i=1}^N \; f_i (\bbsym{x}_i), \\
\text{subject to} & \quad \sum_{i=1}^N \bbsym{A}_{i} \bbsym{x}_i = \bbsym{b}, \\
\end{split}
\end{equation}
where $\XXX_i^{\rm{MI}}$ is the mixed-integer-valued set for $\bbsym{x}_i$,
\[
f_i (\bbsym{x}_i) = \bbsym{x}_i^\top \bbsym{Q}_i \bbsym{x}_i + \bbsym{q}_i^\top \bbsym{x}_i,
\]
is the local objective function of each \agent{i},
$\bbsym{Q}_i$, $\bbsym{q}_i$, $\bbsym{A}_{i}$, and $\bbsym{b}$ are the matrices and vectors of coefficients.
Let $\bbsym{x}^\top = [\bbsym{x}_1^\top, \dots, \bbsym{x}_N^\top]$ be the concatenated vector of optimization variables, and let
$\bbsym{A} = \big[ \bbsym{A}_1, \dots, \bbsym{A}_N \big]$.
\end{itemize}
\end{frame}

\begin{frame}{Distributed Optimization}

\begin{block}{Overview}
\begin{itemize}
\item We combine proximal ADMM with sequential convexification of the integrality constraints.
\item At each iteration $t$, we keep a mixed-integer-valued vector $\bbsym{x}_i^{(t)} \in \XXX_i^{\rm{MI}}$ and an real-valued solution of the relaxed problem $\tilde{\bbsym{x}}_i^{(t)} \in \tilde{\XXX}_i$ where $\tilde{\XXX}_i$ is formed from $\XXX_i$ by relaxing the integrality constraints.
\end{itemize}
\end{block}

\begin{block}{Augmented Lagrangian}
\begin{equation}
\LLL (\bbsym{x}, \bbsym{\lambda}) = \sum_{i=1}^{N} f_i ( \bbsym{x}_i) + \bbsym{\lambda}^\top \bigg(\sum_{i=1}^{N} \bbsym{A}_{i} \bbsym{x}_i - \bbsym{b} \bigg) 
+ \frac{\rho}{2} \norm{\sum_{i=1}^{N} \bbsym{A}_{i} \bbsym{x}_i - \bbsym{b}}_2^2,
\end{equation}
where $\bbsym{\lambda}$ are the dual variables (Lagrangian multipliers), and $\rho > 0$ is a positive constant.
\end{block}

\end{frame}

\begin{frame}{Algorithm}

\begin{block}{Algorithm}
\begin{itemize}
\item \Agent{i} solves the local problem \eqref{eq:local_update}.
% \begin{figure*}
\begin{equation}
\label{eq:local_update}
\bbsym{x}_i^{(t+1)} = \; \underset{\bbsym{x}_i \in \tilde{\XXX}_i}{\argmin}  \; \LLL (\bbsym{x}_i, \tilde{\bbsym{x}}_{-i}^{(t)}, \bbsym{\lambda}^{(t)}) + \beta_i \norm{\bbsym{x}_i - \tilde{\bbsym{x}}_i^{(t)}}_2^2,
\end{equation}
where $\beta_i \in \RRplus$ is a penalty weight.
% \end{figure*}

\item Update the dual variables $\bbsym{\lambda}$ by 
\begin{equation}
\bbsym{\lambda}^{(t+1)} = \bbsym{\lambda}^{(t)} + \gamma \rho \bigg( \sum_{i=1}^{N} \bbsym{A}_{i} \bbsym{x}_i - \bbsym{b} \bigg).
\end{equation}

\item Compute $\tilde{\bbsym{x}}_i^{(t+1)}$ from $\bbsym{x}_i^{(t+1)}$ by rounding operator (transform a real-valued solution into an integer one). 
In other words,
\begin{equation}
\tilde{\bbsym{x}}_i^{(t+1)} = \underset{\bbsym{x}_i \in \XXX_i^{\rm{MI}}}{\argmin} \;
\norm{\bbsym{x}_i - \bbsym{x}_i^{(t+1)}}_2^2,
\end{equation}
\end{itemize}
   
\end{block}
\end{frame}

\begin{frame}{Convergence Analysis}

\begin{block}{Overview}
\begin{itemize}
\item For nonconvex and nonsmooth optimization, to prove convergence, we need to (1) identify a so-called sufficiently decreasing Lyapunov function; and (2) establish the lower boundness property of the Lyapunov function\footcite{yang2022proximal}.
\item Let $(\bbsym{x}^{*}, \bbsym{\lambda}^{*})$ be a saddle point that satisfies the KKT conditions of the relaxed problem (QP)
\begin{equation}
\begin{split}
& \bbsym{A}_i^\top \bbsym{\lambda}^{*} \in \partial f_i(\bbsym{x}_i^{*}),\; \forall i = 1, \dots, N, \\
& \sum_{i=1}^N \bbsym{A}_{i} \bbsym{x}^{*}_i = \bbsym{b}
\end{split}
\end{equation}
where $\partial f_i(\bbsym{x}_i)$ denotes subdifferential of $f_i$ at $\bbsym{x}_i$.
\item We consider the following Lyapunov function
\begin{equation}
\label{eq:Lya-fcn}
\Phi^{(t)} = \norm{\bbsym{x}^{(t)}-\bbsym{x}^{*}}^2_{\bbsym{P}}
+ \frac{1}{\gamma \rho} \norm{\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{*}}_2^2
+ \eta \norm{\xtil^{(t)} - \bbsym{x}^{(t)}}_2^2
\end{equation}
where $\eta > 0$.
\end{itemize}
\end{block}    
\end{frame}

\begin{frame}{Convergence Analysis}
\begin{block}{Lemma 1}
For $t \ge 1$, we have
\begin{equation}
\label{eq:lem1a}
\begin{multlined}
\Big( \norm{\bbsym{x}^{(t)}-\bbsym{x}^{*}}^2_{\bbsym{P}}
+ \frac{1}{\gamma \rho} \norm{\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{*}}_2^2 \Big) 
- \Big(\norm{\bbsym{x}^{(t+1)}-\bbsym{x}^{*}}^2_{\bbsym{P}}
+ \frac{1}{\gamma \rho} \norm{\bbsym{\lambda}^{(t+1)}-\bbsym{\lambda}^{*}}_2^2 \Big) \\
\ge \norm{\xtil^{(t)}-\bbsym{x}^{(t+1)}}_{\bbsym{P}}^2
+ \frac{2-\gamma}{\rho \gamma^2} \norm{\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{(t+1)}}_2^2 
+ \frac{2}{\gamma} (\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{(t+1)})^\top \bbsym{A} (\xtil^{(t)}-\bbsym{x}^{(t+1)}).
\end{multlined}
\end{equation}
\end{block}

\end{frame}

\begin{frame}{Convergence Analysis}
\begin{block}{Lemma 2}
For $t \ge 1$, we have
\begin{equation}
\label{eq:fcn_decrease}
\begin{multlined}
\Phi^{(t)} - \Phi^{(t+1)}  
\ge \norm{\xtil^{(t)}-\bbsym{x}^{(t+1)}}_{\bbsym{P}-\eta \II}^2
+ \frac{2-\gamma}{\rho \gamma^2} \norm{\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{(t+1)}}_2^2 \\
+ \frac{2}{\gamma} (\bbsym{\lambda}^{(t)}-\bbsym{\lambda}^{(t+1)})^\top \bbsym{A} (\xtil^{(t)}-\bbsym{x}^{(t+1)})
+ \eta \norm{\xtil^{(t)}-\bbsym{x}^{(t)}}_2^2
\end{multlined}
\end{equation}
As a result, if the matrix 
\begin{equation}
\bbsym{R} = 
\begin{pmatrix}
\bbsym{P} - \eta \II & \quad \frac{1}{\rho} \bbsym{A}^\top \\
\frac{1}{\rho} \bbsym{A} & \quad \frac{2-\gamma}{\rho \gamma^2} \II \\
\end{pmatrix}
\end{equation}
is positive definite, then $\{\Phi^{(t)}\}$ sufficiently decrease.
\end{block}
\end{frame}

\begin{frame}{Convergence Analysis}
\begin{block}{Theorem}
If the parameters $\rho$, $\gamma$, and $\beta$ are chosen such that the following conditions are satisfied
\begin{equation}
\label{eq:conditions}
\begin{split}
& \beta_i > \eta + \rho\, (1/\epsilon-1)\, e_i, \\
& 2-\gamma > N \epsilon,
\end{split}
\end{equation}
where $e_i$ is the maximum eigenvalue of $\bbsym{A}_i^\top \bbsym{A}_i$, $\epsilon > 0$ is a positive constant,
then $\{\bbsym{x}^{(t)} - \tilde{\bbsym{x}}^{(t)}\}$ converge to 0, and the sequence $\{\bbsym{x}^{(t)} , \bbsym{\lambda}^{(t)}\}$ converges.
\end{block}
\end{frame}

\end{document}
        